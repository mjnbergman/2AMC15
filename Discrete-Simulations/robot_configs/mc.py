######
# Policy iteratation agent:
#  Makes a move based on policy generated by the policy iteration algorithm
#  Main loop can be found in robot_epoch
######

import numpy as np

# Reward map that provides the scores associated with tiles. -3<=current position, -2&-1=walls&obstacles, 0=clean, 1=dirty, 2=goal
simple_reward_map = {-6: -2, -5: -2, -4: -2, -3: -2, -2: -9, -1: -9, 0: -2, 1: 2, 2: 4, 3: -10}

# Discount factor
gamma = 1

error = .5

Q = None
POLICY = np.array([])
RETURNS = np.array([])

REWARDS = [0]
PATH = []


def get_actions(x, y, robot, grid, values=[]):
    """
    Returns the possible tiles (actions) for a robot position. Skips the current tile, unless it 
    is the only available tile (see report).

    @robot: robot that is on the requested position
    @grid: original grid with tile value
    @values: values calculated in the previous iterations

    """
    data = {}

    # Check for all moves if they are possible tiles/actions
    for move in list(robot.dirs.values()):
        move_coord = tuple(np.array((x, y)) + (np.array(move)))

        # Filter out out of bounds tiles
        if move_coord[0] < robot.grid.cells.shape[0] and move_coord[1] < robot.grid.cells.shape[1] and move_coord[0] >= 0 and move_coord[1] >= 0:

            # Skip walls, obstacles and current position. Save latter for if no other tiles are found.
            if grid[move_coord] > -3 and grid[move_coord] < 0:
                continue

            if len(values) != 0:
                data[tuple(np.array(move))] = values[(x, y)][move]
            else:
                data[tuple(np.array(move))] = {}
    return data


def e_soft(actions, error, greedy=False):
    greedy_part = 0

    if greedy:
        greedy_part = 1-error

    return greedy_part + error/len(actions)


def gen_policy(robot, grid):
    """
    Generates a random policy to start in which walls and obstacles
    have no policy and can't be the suggested policy of another state


    @robot: original robot that is on its actual position
    @grid: original grid with tile value

    """

    policy = np.empty_like(grid, dtype=object)

    # Loops over entire grid
    for x in range(0, len(grid)):
        for y in range(0, len(grid[0])):
           # Skip walls and obstacles, no policy is given instead '-'
            if grid[(x, y)] < 0 and grid[(x, y)] > -3:
                policy[x, y] = '-'
                continue

            actions = get_actions(x, y, robot, grid)
            policy[x, y] = {a: e_soft(actions, 1, greedy=False) for a in actions}

    return policy


def robot_epoch(robot):
    """
    Makes a move for the robot based on the policy generated
    in three steps. Random policy initialization and then repeating steps
    of policy evaluation and improvement until the policy is stable

    @robot: original robot that is on its actual position
    """

    global POLICY
    global Q
    global RETURNS
    global REWARDS
    global PATH

    # Initializes polocy, values and returns (for each Q(S,a) pair)
    if POLICY.shape[0] == 0:
        POLICY = gen_policy(robot, robot.grid.cells)
        Q = np.array([[{action: 0 for action in get_actions(x, y, robot, robot.grid.cells)} for y, cell in enumerate(row)] for x, row in enumerate(robot.grid.cells)])
        RETURNS = np.array([[{action: [] for action in get_actions(x, y, robot, robot.grid.cells)} for y, cell in enumerate(row)] for x, row in enumerate(robot.grid.cells)])

    # Pick action based on policy probabilities
    ri = np.random.choice(np.arange(0, len(POLICY[robot.pos])), p=list(POLICY[robot.pos].values()))
    action = list(POLICY[robot.pos].keys())[ri]
    action_coords = tuple(np.array(robot.pos) + (np.array(action)))

    # Calculate returns
    G = gamma*REWARDS[-1] + simple_reward_map[robot.grid.cells[action_coords]]

    PATH.append([robot.pos, action])
    REWARDS.append(G)

    new_orient = list(robot.dirs.keys())[list(robot.dirs.values()).index(action)]

    # Orient ourselves towards the dirty tile:
    while new_orient != robot.orientation:
        robot.rotate('r')

    robot.move()

    # Policy Improvement
    # Update Policy for each square on which a move is made
    if not robot.alive:
        for [(x, y), action] in PATH:
            # Where in path is first occurence
            path_index = PATH.index([(x, y), action])

            # Calculate returns after first occurence
            RETURNS[x, y][action].append(REWARDS[-1] - REWARDS[path_index])

            Q[x, y][action] = np.mean(RETURNS[x, y][action])

            action_values = get_actions(x, y, robot, robot.grid.cells, Q)
            max_action = max(action_values, key=action_values.get)

            POLICY[x, y] = {action: e_soft(POLICY[x, y].keys(), error, action == max_action) for action in POLICY[x, y].keys()}

        PATH = []
        REWARDS = [0]
        G = 0
        print("\n 1, 2")
        print(get_actions(1, 2, robot, robot.grid.cells, Q))
        print(POLICY[1, 2])
        print("\n 2, 1")

        print(get_actions(2, 1, robot, robot.grid.cells, Q))
        print(POLICY[2, 1])